map(allData, glimpse)
library(purrr)
map(allData, glimpse)
glimpse(allData[[15]])
glimpse(allData[[12]])
glimpse(allData[[12]])
class(allData[[12]])
class(allData[[12]] %>% data.frame)
glimpse(allData[[12]] %>% data.frame)
library(httr)
library(jsonlite)
library(lubridate)
library(dplyr)
source("D:/Projects/creds.R")
# access News API (https://newsapi.org/) to get headline articles
sources <- c("associated-press","bbc-news","bloomberg","business-insider",
"buzzfeed","cnbc","cnn","google-news","independent","reuters",
"the-economist", "the-huffington-post","the-new-york-times",
"the-wall-street-journal", "the-washington-post","time","usa-today")
requestLinks <- paste0("https://newsapi.org/v1/articles?source=",
sources,"&apiKey=", news.API.key)
# for sorting: "&sortBy=latest"
allData <- vector("list", length(sources))
for(x in 1:length(sources)){
news.source <- GET(requestLinks[x])
news.source <- content(news.source)
news.source <- fromJSON(toJSON(news.source))
allData[[x]] <- news.source}
# 'time magazine' has diff data structure
allData[[16]]$articles.publishedAt <- NULL
allData[[16]]$articles.author <- NULL
for(x in 1:length(allData)){
allData[[x]] <- data.frame(allData[[x]])}
summary(allData)
a <- allData %>%
Reduce(function(d1, d2) full_join(d1,d2), .)
map(allData, class)
map(allData, function(x) map(x, class))
a1 <- map(allData, names)
a1
a1 <- map(allData, function(x) map(x, class))
a1
seq_along(allData)
a <- alldata[[3]]
a <- allData[[3]]
a
glimpse(a)
a$articles.author
glimpse(a)
a$articles.author %>% unlist
a$articles.author %>% unlist %>% length
a$articles.title %>% unlist %>% length
a$articles.description %>% unlist %>% length
a$articles.publishedAt %>% unlist %>% length
a$articles.publishedAt %>% unlist
glimpse(a)
library(httr)
library(jsonlite)
library(lubridate)
library(dplyr)
source("D:/Projects/creds.R")
# access News API (https://newsapi.org/) to get headline articles
sources <- c("associated-press","bbc-news","bloomberg","business-insider",
"buzzfeed","cnbc","cnn","google-news","independent","reuters",
"the-economist", "the-huffington-post","the-new-york-times",
"the-wall-street-journal", "the-washington-post","time","usa-today")
requestLinks <- paste0("https://newsapi.org/v1/articles?source=",
sources,"&apiKey=", news.API.key)
# for sorting: "&sortBy=latest"
allData <- vector("list", length(sources))
for(x in 1:length(sources)){
news.source <- GET(requestLinks[x])
news.source <- content(news.source)
news.source <- fromJSON(toJSON(news.source))
allData[[x]] <- news.source
allData[[x]] <- data.frame(allData[[x]])
allData[[x]] <- allData[[x]] %>%
select(-articles.author)}
glimpse(allData)
summary(allData)
a <- allData %>%
Reduce(function(d1, d2) full_join(d1, d2), .)
glimpse(a)
library(httr)
library(jsonlite)
library(lubridate)
library(dplyr)
source("D:/Projects/creds.R")
# access News API (https://newsapi.org/) to get headline articles
sources <- c("associated-press","bbc-news","bloomberg","business-insider",
"buzzfeed","cnbc","cnn","google-news","independent","reuters",
"the-economist", "the-huffington-post","the-new-york-times",
"the-wall-street-journal", "the-washington-post","time","usa-today")
requestLinks <- paste0("https://newsapi.org/v1/articles?source=",
sources,"&apiKey=", news.API.key)
# for sorting: "&sortBy=latest"
allData <- vector("list", length(sources))
for(x in 1:length(sources)){
news.source <- GET(requestLinks[x])
news.source <- content(news.source)
news.source <- fromJSON(toJSON(news.source))
allData[[x]] <- news.source
allData[[x]] <- data.frame(allData[[x]])
allData[[x]] <- allData[[x]] %>%
select(-articles.author)
allData[[x]]$articles.title <- allData[[x]]$articles.title %>% unlist
allData[[x]]$articles.description <- allData[[x]]$articles.description %>% unlist
allData[[x]]$articles.url <- allData[[x]]$articles.urlToImage %>% unlist
allData[[x]]$articles.publishedAt <- allData[[x]]$articles.publishedAt %>% unlist}
x
library(httr)
library(jsonlite)
library(lubridate)
library(dplyr)
source("D:/Projects/creds.R")
# access News API (https://newsapi.org/) to get headline articles
sources <- c("associated-press","bbc-news","bloomberg","business-insider",
"buzzfeed","cnbc","cnn","google-news","independent","reuters",
"the-economist", "the-huffington-post","the-new-york-times",
"the-wall-street-journal", "the-washington-post","time","usa-today")
requestLinks <- paste0("https://newsapi.org/v1/articles?source=",
sources,"&apiKey=", news.API.key)
# for sorting: "&sortBy=latest"
allData <- vector("list", length(sources))
for(x in 1:length(sources)){
news.source <- GET(requestLinks[x])
news.source <- content(news.source)
news.source <- fromJSON(toJSON(news.source))
allData[[x]] <- news.source
allData[[x]] <- data.frame(allData[[x]])
allData[[x]] <- allData[[x]] %>%
select(-articles.author)
allData[[x]]$articles.title <- allData[[x]]$articles.title %>% unlist
#allData[[x]]$articles.description <- allData[[x]]$articles.description %>% unlist
allData[[x]]$articles.url <- allData[[x]]$articles.urlToImage %>% unlist}
#allData[[x]]$articles.publishedAt <- allData[[x]]$articles.publishedAt %>% unlist}
a <- allData %>%
Reduce(function(d1, d2) full_join(d1, d2), .)
library(httr)
library(jsonlite)
library(lubridate)
library(dplyr)
source("D:/Projects/creds.R")
# access News API (https://newsapi.org/) to get headline articles
sources <- c("associated-press","bbc-news","bloomberg","business-insider",
"buzzfeed","cnbc","cnn","google-news","independent","reuters",
"the-economist", "the-huffington-post","the-new-york-times",
"the-wall-street-journal", "the-washington-post","time","usa-today")
requestLinks <- paste0("https://newsapi.org/v1/articles?source=",
sources,"&apiKey=", news.API.key)
# for sorting: "&sortBy=latest"
allData <- vector("list", length(sources))
for(x in 1:length(sources)){
news.source <- GET(requestLinks[x])
news.source <- content(news.source)
news.source <- fromJSON(toJSON(news.source))
allData[[x]] <- news.source
allData[[x]] <- data.frame(allData[[x]])
allData[[x]] <- allData[[x]] %>%
select(-articles.author)
allData[[x]]$articles.title <- allData[[x]]$articles.title %>% unlist
allData[[x]]$articles.description <- allData[[x]]$articles.description %>% unlist
allData[[x]]$articles.url <- allData[[x]]$articles.urlToImage %>% unlist}
#allData[[x]]$articles.publishedAt <- allData[[x]]$articles.publishedAt %>% unlist}
library(httr)
library(jsonlite)
library(lubridate)
library(dplyr)
source("D:/Projects/creds.R")
# access News API (https://newsapi.org/) to get headline articles
sources <- c("associated-press","al-jazeera-english","bbc-news","bloomberg",
"business-insider","breitbart-news","cnbc","cnn","google-news",
"independent","reuters","the-economist", "the-huffington-post",
"newsweek","the-new-york-times","the-wall-street-journal",
"the-washington-post","time","usa-today")
requestLinks <- paste0("https://newsapi.org/v1/articles?source=",
sources,"&apiKey=", news.API.key)
# for sorting: "&sortBy=latest"
allData <- vector("list", length(sources))
for(x in 1:length(sources)){
news.source <- GET(requestLinks[x])
news.source <- content(news.source)
news.source <- fromJSON(toJSON(news.source))
allData[[x]] <- news.source
allData[[x]] <- data.frame(allData[[x]])
allData[[x]] <- allData[[x]] %>%
select(-articles.author)
allData[[x]]$articles.title <- allData[[x]]$articles.title %>% unlist
allData[[x]]$articles.description <- allData[[x]]$articles.description %>% unlist
allData[[x]]$articles.url <- allData[[x]]$articles.urlToImage %>% unlist}
library(httr)
library(jsonlite)
library(lubridate)
library(dplyr)
source("D:/Projects/creds.R")
# access News API (https://newsapi.org/) to get headline articles
sources <- c("associated-press","al-jazeera-english","bbc-news","bloomberg",
"business-insider","breitbart-news","cnbc","cnn","google-news",
"independent","reuters","the-economist", "the-huffington-post",
"newsweek","the-new-york-times","the-wall-street-journal",
"the-washington-post","time","usa-today")
requestLinks <- paste0("https://newsapi.org/v1/articles?source=",
sources,"&apiKey=", news.API.key)
# for sorting: "&sortBy=latest"
allData <- vector("list", length(sources))
for(x in 1:length(sources)){
news.source <- GET(requestLinks[x])
news.source <- content(news.source)
news.source <- fromJSON(toJSON(news.source))
allData[[x]] <- news.source}
allData %>% summary
a <- allData[[1]]
a
tibble(a)
b <- tibble(a)
b
View(b)
?unnest
View(allData[[1]])
View(allData[[2]])
allData[[1]] %>% class
allData[[1]] %>% data.frame %>% glimpse
View(allData[[3]])
allData[[3]]$articles
allData[[3]]$articles %>% class
allData[[1]] %>% class
allData[[1]] %>% length
allData[[1]] %>% attr
map_chr(allData[[1]], class)
map_chr(allData[[1]], dim)
map(allData[[1]], dim)
map(allData[[1]], length)
news.source
news.source$status
news.source$articles %>% glimpse
news.source$articles %>% data.frame %>% glimpse
library(httr)
library(jsonlite)
library(lubridate)
library(dplyr)
source("D:/Projects/creds.R")
# access News API (https://newsapi.org/) to get headline articles
sources <- c("associated-press","al-jazeera-english","bbc-news","bloomberg",
"business-insider","breitbart-news","cnbc","cnn","google-news",
"independent","reuters","the-economist", "the-huffington-post",
"newsweek","the-new-york-times","the-wall-street-journal",
"the-washington-post","time","usa-today")
requestLinks <- paste0("https://newsapi.org/v1/articles?source=",
sources,"&apiKey=", news.API.key)
# for sorting: "&sortBy=latest"
allData <- vector("list", length(sources))
for(x in 1:length(sources)){
news.source <- GET(requestLinks[x])
news.source <- content(news.source)
news.source <- fromJSON(toJSON(news.source))
allData[[x]] <- news.source}
seq_along(sources)
1:seq_along(sources)
news.source <- allData[[5]]
news.source$status
df <- news.source$articles
df$status <- rep(news.source$status, nrow(df))
df$source <- rep(news.source$source, nrow(df))
df$sortBy <- rep(news.source$sortBy, nrow(df))
glimpse(df)
df <- news.source$articles
glimpse(df)
map(df, unlist) %>% glimpse
map(df, unlist)
map(df, unlist) %>% data.frame %>% glimpse
map(df, unlist) %>% data.frame %>% View
library(httr)
library(jsonlite)
library(lubridate)
library(dplyr)
source("D:/Projects/creds.R")
# access News API (https://newsapi.org/) to get headline articles
sources <- c("associated-press","al-jazeera-english","bbc-news","bloomberg",
"business-insider","breitbart-news","cnbc","cnn","google-news",
"independent","reuters","the-economist", "the-huffington-post",
"newsweek","the-new-york-times","the-wall-street-journal",
"the-washington-post","time","usa-today")
requestLinks <- paste0("https://newsapi.org/v1/articles?source=",
sources,"&apiKey=", news.API.key)
# for sorting: "&sortBy=latest"
# loop to retrieve JSON news data into initialized list
allData <- vector("list", length(sources))
for(x in seq_along(sources)){
news.source <- GET(requestLinks[x])
news.source <- content(news.source)
news.source <- fromJSON(toJSON(news.source))
allData[[x]] <- news.source}
# convert list objects to dataframes
for(x in seq_along(sources)){
news.source <- allData[[x]]
df <- news.source$articles
df <- df %>%
map(unlist) %>%
data.frame
df$status <- rep(news.source$status, nrow(df))
df$source <- rep(news.source$source, nrow(df))
df$sortBy <- rep(news.source$sortBy, nrow(df))
allData[[x]] <- df
}
library(httr)
library(jsonlite)
library(lubridate)
library(dplyr)
source("D:/Projects/creds.R")
# access News API (https://newsapi.org/) to get headline articles
sources <- c("associated-press","al-jazeera-english","bbc-news","bloomberg",
"business-insider","breitbart-news","cnbc","cnn","google-news",
"independent","reuters","the-economist", "the-huffington-post",
"newsweek","the-new-york-times","the-wall-street-journal",
"the-washington-post","time","usa-today")
requestLinks <- paste0("https://newsapi.org/v1/articles?source=",
sources,"&apiKey=", news.API.key)
# for sorting: "&sortBy=latest"
# loop to retrieve JSON news data into initialized list
allData <- vector("list", length(sources))
for(x in seq_along(sources)){
news.source <- GET(requestLinks[x])
news.source <- content(news.source)
news.source <- fromJSON(toJSON(news.source))
allData[[x]] <- news.source}
# convert list objects to dataframes
for(x in seq_along(sources)){
news.source <- allData[[x]]
df <- news.source$articles
df <- df %>%
map(unlist) %>%
data.frame
#df$status <- rep(news.source$status, nrow(df))
#df$source <- rep(news.source$source, nrow(df))
#df$sortBy <- rep(news.source$sortBy, nrow(df))
allData[[x]] <- df
}
x
news.source <- allData[[3]]
df <- news.source$articles
df <- df %>%
map(unlist) %>%
data.frame
View(df)
df$publishedAt %>% nchar
df$publishedAt[df$publishedAt %>% nchar < 10]
df$publishedAt[df$publishedAt %>% nchar < 10] <- NA
df <- df %>%
map(unlist) %>%
data.frame
library(httr)
library(jsonlite)
library(lubridate)
library(dplyr)
source("D:/Projects/creds.R")
# access News API (https://newsapi.org/) to get headline articles
sources <- c("associated-press","al-jazeera-english","bbc-news","bloomberg",
"business-insider","breitbart-news","cnbc","cnn","google-news",
"independent","reuters","the-economist", "the-huffington-post",
"newsweek","the-new-york-times","the-wall-street-journal",
"the-washington-post","time","usa-today")
requestLinks <- paste0("https://newsapi.org/v1/articles?source=",
sources,"&apiKey=", news.API.key)
# for sorting: "&sortBy=latest"
# loop to retrieve JSON news data into initialized list
allData <- vector("list", length(sources))
for(x in seq_along(sources)){
news.source <- GET(requestLinks[x])
news.source <- content(news.source)
news.source <- fromJSON(toJSON(news.source))
allData[[x]] <- news.source}
# convert list objects to dataframes
for(x in seq_along(sources)){
news.source <- allData[[3]]
df <- news.source$articles
df$publishedAt[df$publishedAt %>% nchar < 25] <- NA # hax
df <- df %>%
map(unlist) %>%
data.frame
#df$status <- rep(news.source$status, nrow(df))
#df$source <- rep(news.source$source, nrow(df))
#df$sortBy <- rep(news.source$sortBy, nrow(df))
allData[[x]] <- df
}
summary(allData)
map(allData, function(x) map_chr(x, class))
library(httr)
library(jsonlite)
library(lubridate)
library(dplyr)
source("D:/Projects/creds.R")
# access News API (https://newsapi.org/) to get headline articles
sources <- c("associated-press","al-jazeera-english","bbc-news","bloomberg",
"business-insider","breitbart-news","cnbc","cnn","google-news",
"independent","reuters","the-economist", "the-huffington-post",
"newsweek","the-new-york-times","the-wall-street-journal",
"the-washington-post","time","usa-today")
requestLinks <- paste0("https://newsapi.org/v1/articles?source=",
sources,"&apiKey=", news.API.key)
# for sorting: "&sortBy=latest"
# loop to retrieve JSON news data into initialized list
allData <- vector("list", length(sources))
for(x in seq_along(sources)){
news.source <- GET(requestLinks[x])
news.source <- content(news.source)
news.source <- fromJSON(toJSON(news.source))
allData[[x]] <- news.source}
# convert list objects to dataframes
for(x in seq_along(sources)){
news.source <- allData[[3]]
df <- news.source$articles
df$publishedAt[df$publishedAt %>% nchar < 10] <- NA # hax
df <- df %>%
map(unlist) %>%
data.frame
#df$status <- rep(news.source$status, nrow(df))
#df$source <- rep(news.source$source, nrow(df))
#df$sortBy <- rep(news.source$sortBy, nrow(df))
allData[[x]] <- df
}
map(allData, function(x) map_chr(x, class))
allData[[19]]
allData[[1]]
library(httr)
library(jsonlite)
library(lubridate)
library(dplyr)
source("D:/Projects/creds.R")
# access News API (https://newsapi.org/) to get headline articles
sources <- c("associated-press","al-jazeera-english","bbc-news","bloomberg",
"business-insider","breitbart-news","cnbc","cnn","google-news",
"independent","reuters","the-economist", "the-huffington-post",
"newsweek","the-new-york-times","the-wall-street-journal",
"the-washington-post","time","usa-today")
requestLinks <- paste0("https://newsapi.org/v1/articles?source=",
sources,"&apiKey=", news.API.key)
# for sorting: "&sortBy=latest"
# loop to retrieve JSON news data into initialized list
allData <- vector("list", length(sources))
for(x in seq_along(sources)){
news.source <- GET(requestLinks[x])
news.source <- content(news.source)
news.source <- fromJSON(toJSON(news.source))
allData[[x]] <- news.source}
# convert list objects to dataframes
for(x in seq_along(sources)){
news.source <- allData[[3]]
df <- news.source$articles
df$publishedAt[df$publishedAt == "list()"] <- NA
df <- df %>%
map(unlist) %>%
data.frame
#df$status <- rep(news.source$status, nrow(df))
#df$source <- rep(news.source$source, nrow(df))
#df$sortBy <- rep(news.source$sortBy, nrow(df))
allData[[x]] <- df
}
map(allData, function(x) map_chr(x, class))
library(httr)
library(jsonlite)
library(lubridate)
library(dplyr)
source("D:/Projects/creds.R")
# access News API (https://newsapi.org/) to get headline articles
sources <- c("associated-press","al-jazeera-english","bbc-news","bloomberg",
"business-insider","breitbart-news","cnbc","cnn","google-news",
"independent","reuters","the-economist", "the-huffington-post",
"newsweek","the-new-york-times","the-wall-street-journal",
"the-washington-post","time","usa-today")
requestLinks <- paste0("https://newsapi.org/v1/articles?source=",
sources,"&apiKey=", news.API.key)
# for sorting: "&sortBy=latest"
# loop to retrieve JSON news data into initialized list
allData <- vector("list", length(sources))
for(x in seq_along(sources)){
news.source <- GET(requestLinks[x])
news.source <- content(news.source)
news.source <- fromJSON(toJSON(news.source))
allData[[x]] <- news.source}
allData[[1]]
allData[[4]]
allData[[5]]
allData[[6]]
allData[[6]]$articles
allData[[6]]$articles$ publishedAt
allData[[6]]$articles$ publishedAt %>% unlist %>% length
x
news.source <- GET(requestLinks[x])
news.source <- content(news.source)
news.source %>% class
news.source
news.source %>% summary
news.source %>% fromJSON %>% class
news.source <- GET(requestLinks[x])
news.source
news.source %>% class
news.source <- content(news.source)
news.source %>% class
news.source %>% class
news.source %>% length
news.source %>% summary
news.source %>% articles
news.source$articles
news.source$articles %>% class
news.source$articles %>% glimpse
news.source$articles %>% fromJSON %>% glimpse
news.source$articles %>% toJSON
news.source$articles %>% class
news.source$articles %>% map %>% names
news.source$articles %>% map(names)
news.source$articles %>% map(class)
news.source$articles %>% map(function(x) map(x, length))
